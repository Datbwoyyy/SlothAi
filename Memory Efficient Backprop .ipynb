{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpm+hQEOnlV5doIZcKZHPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datbwoyyy/SlothAi/blob/main/Untitled16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlFb2yyLLM_v",
        "outputId": "5acfce14-f7b7-493f-8fd0-a4edef6ac528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 (Cross Entropy): Loss = 131.125\n",
            "Example 2 (Sigmoid + MSE): Loss = 1.4853515625\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "import torch.utils.checkpoint as checkpoint  # Correct import\n",
        "\n",
        "# Use torch.cuda.amp.autocast if a GPU is available; otherwise, create a dummy context.\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda.amp import autocast\n",
        "else:\n",
        "    @contextmanager\n",
        "    def autocast():\n",
        "        yield\n",
        "\n",
        "class MemoryEfficientFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, W, transform_fn):\n",
        "        \"\"\"\n",
        "        Forward pass using PyTorch checkpointing to save memory.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(X, W)\n",
        "        ctx.transform_fn = transform_fn\n",
        "\n",
        "        def chunk_forward(x):\n",
        "            \"\"\"Compute logits & apply transformation for a given chunk.\"\"\"\n",
        "            logits = x.matmul(W)  # X @ W (Matrix Multiplication)\n",
        "            return transform_fn(logits)  # Apply activation (log_softmax, sigmoid, etc.)\n",
        "\n",
        "        # Use torch.utils.checkpoint to save memory\n",
        "        # Use preserve_rng_state=False as it is deprecated\n",
        "        output = checkpoint.checkpoint(chunk_forward, X, use_reentrant=False)\n",
        "        return output  # Correctly return the computed output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: Recompute forward function using checkpointing and propagate gradients.\n",
        "        \"\"\"\n",
        "        X, W = ctx.saved_tensors\n",
        "        transform_fn = ctx.transform_fn\n",
        "\n",
        "        # Recompute the forward pass\n",
        "        def chunk_forward(x):\n",
        "            logits = x.matmul(W)\n",
        "            return transform_fn(logits)\n",
        "\n",
        "        # X.requires_grad_(True)  # This is redundant as X already has requires_grad=True from forward\n",
        "        with torch.enable_grad(): # Enable gradient computation within this block\n",
        "            logits_recomputed = chunk_forward(X)  # Recompute forward pass\n",
        "\n",
        "        # Compute gradients\n",
        "        grad_X, grad_W = torch.autograd.grad(logits_recomputed, (X, W), grad_outputs=grad_output, retain_graph=True)\n",
        "\n",
        "        return grad_X, grad_W, None\n",
        "\n",
        "# Convenience wrapper.\n",
        "def memory_efficient_forward(X, W, transform_fn):\n",
        "    return MemoryEfficientFunction.apply(X, W, transform_fn)\n",
        "\n",
        "# --- Example Usages ---\n",
        "\n",
        "# Example 1: Cross Entropy Loss (using log softmax)\n",
        "def example_cross_entropy():\n",
        "    bsz, qlen, hd, vocab = 4, 1024, 1024, 32000  # Adjusted for Colab\n",
        "    X = torch.randn(bsz * qlen, hd, dtype=torch.float16, requires_grad=True)\n",
        "    W = torch.randn(hd, vocab, dtype=torch.float16, requires_grad=True)\n",
        "    targets = torch.randint(0, vocab, (bsz * qlen,))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        X, W, targets = X.cuda(), W.cuda(), targets.cuda()\n",
        "\n",
        "    transform_fn = lambda logits: F.log_softmax(logits, dim=-1)  # Log Softmax for Cross Entropy\n",
        "    output = memory_efficient_forward(X, W, transform_fn)\n",
        "    loss = F.nll_loss(output, targets)\n",
        "    loss.backward()\n",
        "    print(\"Example 1 (Cross Entropy): Loss =\", loss.item())\n",
        "\n",
        "# Example 2: Sigmoid Activation with MSE Loss\n",
        "def example_sigmoid_mse():\n",
        "    bsz, qlen, hd, vocab = 4, 1024, 1024, 32000\n",
        "    X = torch.randn(bsz * qlen, hd, dtype=torch.float16, requires_grad=True)\n",
        "    W = torch.randn(hd, vocab, dtype=torch.float16, requires_grad=True)\n",
        "    targets = torch.randn(bsz * qlen, vocab, dtype=torch.float16)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        X, W, targets = X.cuda(), W.cuda(), targets.cuda()\n",
        "\n",
        "    transform_fn = torch.sigmoid  # Sigmoid Activation\n",
        "    output = memory_efficient_forward(X, W, transform_fn)\n",
        "    loss = F.mse_loss(output, targets)\n",
        "    loss.backward()\n",
        "    print(\"Example 2 (Sigmoid + MSE): Loss =\", loss.item())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    example_cross_entropy()\n",
        "    example_sigmoid_mse()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "from contextlib import contextmanager\n",
        "import torch.utils.checkpoint as checkpoint  # Correct import\n",
        "\n",
        "# Use torch.cuda.amp.autocast if a GPU is available; otherwise, create a dummy context.\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda.amp import autocast\n",
        "else:\n",
        "    @contextmanager\n",
        "    def autocast():\n",
        "        yield\n",
        "\n",
        "# Enable TF32 for faster matmul on NVIDIA GPUs\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "class MemoryEfficientFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, W, transform_fn):\n",
        "        \"\"\"\n",
        "        Forward pass using PyTorch checkpointing to save memory.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(X, W)\n",
        "        ctx.transform_fn = transform_fn\n",
        "\n",
        "        def chunk_forward(x):\n",
        "            \"\"\"Compute logits & apply transformation for a given chunk.\"\"\"\n",
        "            logits = x.matmul(W)  # X @ W (Matrix Multiplication)\n",
        "            return transform_fn(logits)  # Apply activation (log_softmax, sigmoid, etc.)\n",
        "\n",
        "        # Use torch.utils.checkpoint to save memory\n",
        "        output = checkpoint.checkpoint(chunk_forward, X, use_reentrant=False)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: Recompute forward function using checkpointing and propagate gradients.\n",
        "        \"\"\"\n",
        "        X, W = ctx.saved_tensors\n",
        "        transform_fn = ctx.transform_fn\n",
        "\n",
        "        # Recompute the forward pass\n",
        "        def chunk_forward(x):\n",
        "            logits = x.matmul(W)\n",
        "            return transform_fn(logits)\n",
        "\n",
        "        # Create a detached copy of X that requires grad (ensuring it's a leaf variable)\n",
        "        with torch.enable_grad():\n",
        "            X_ = X.detach().requires_grad_(True)\n",
        "            logits_recomputed = checkpoint.checkpoint(chunk_forward, X_, use_reentrant=False)\n",
        "\n",
        "        # Compute gradients with respect to X_ and W\n",
        "        grad_X, grad_W = torch.autograd.grad(\n",
        "            logits_recomputed,\n",
        "            (X_, W),\n",
        "            grad_outputs=grad_output,\n",
        "            retain_graph=False\n",
        "        )\n",
        "\n",
        "        return grad_X, grad_W, None\n",
        "\n",
        "# Convenience wrapper.\n",
        "def memory_efficient_forward(X, W, transform_fn):\n",
        "    return MemoryEfficientFunction.apply(X, W, transform_fn)\n",
        "\n",
        "# --- Example Usages ---\n",
        "def get_dtype():\n",
        "    return torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16\n",
        "\n",
        "# Example 1: Cross Entropy Loss (using log softmax)\n",
        "def example_cross_entropy():\n",
        "    bsz, qlen, hd, vocab = 4, 1024, 1024, 32000  # Adjusted for Colab\n",
        "    dtype = get_dtype()\n",
        "    X = torch.randn(bsz * qlen, hd, dtype=dtype, requires_grad=True)\n",
        "    W = torch.randn(hd, vocab, dtype=dtype, requires_grad=True)\n",
        "    targets = torch.randint(0, vocab, (bsz * qlen,))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        X, W, targets = X.cuda(), W.cuda(), targets.cuda()\n",
        "\n",
        "    transform_fn = lambda logits: F.log_softmax(logits, dim=-1)  # Log Softmax for Cross Entropy\n",
        "    output = memory_efficient_forward(X, W, transform_fn)\n",
        "    loss = F.nll_loss(output, targets)\n",
        "    loss.backward()\n",
        "    print(\"Example 1 (Cross Entropy): Loss =\", loss.item())\n",
        "\n",
        "# Example 2: Sigmoid Activation with MSE Loss\n",
        "def example_sigmoid_mse():\n",
        "    bsz, qlen, hd, vocab = 4, 1024, 1024, 32000\n",
        "    dtype = get_dtype()\n",
        "    X = torch.randn(bsz * qlen, hd, dtype=dtype, requires_grad=True)\n",
        "    W = torch.randn(hd, vocab, dtype=dtype, requires_grad=True)\n",
        "    targets = torch.randn(bsz * qlen, vocab, dtype=dtype)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        X, W, targets = X.cuda(), W.cuda(), targets.cuda()\n",
        "\n",
        "    transform_fn = torch.sigmoid  # Sigmoid Activation\n",
        "    output = memory_efficient_forward(X, W, transform_fn)\n",
        "    loss = F.mse_loss(output, targets)\n",
        "    loss.backward()\n",
        "    print(\"Example 2 (Sigmoid + MSE): Loss =\", loss.item())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    example_cross_entropy()\n",
        "    example_sigmoid_mse()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvxaC4gpV_6T",
        "outputId": "33f581b4-2b41-4cb0-902e-0467eee4ba2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 (Cross Entropy): Loss = 131.875\n",
            "Example 2 (Sigmoid + MSE): Loss = 1.4853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HpWY2oZUddms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ID34kI4a48T8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
