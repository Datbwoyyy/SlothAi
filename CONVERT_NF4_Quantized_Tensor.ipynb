{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4858tvQSy3LiNU42OV2NG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datbwoyyy/SlothAi/blob/main/CONVERT_NF4_Quantized_Tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Convert** a nf4 quantized tensor into fp16 or bf16 into a **single Triton kernel**"
      ],
      "metadata": {
        "id": "BnvFWBCofEYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgSHfR1dd562",
        "outputId": "80f0000a-0b7e-4b9e-d8e5-18f436e97495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match: True\n",
            "Triton Speedup: 1.03x\n"
          ]
        }
      ],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "@triton.jit\n",
        "def nf4_dequant_kernel(weight_ptr, absmax_ptr, out_ptr,\n",
        "                       M: int, N: int, stride_row: int, stride_col: int,\n",
        "                       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n",
        "    # Compute row and column indices of the block this program instance handles\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    # Use tl.arange instead of tl.static_range for creating ranges that can be used in arithmetic operations\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  # row indices\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)  # column indices\n",
        "\n",
        "    # Create a 2D mask to avoid out–of–bounds accesses\n",
        "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n",
        "\n",
        "    # Load a tile of nf4 weights\n",
        "    weight_tile_ptr = weight_ptr + offs_m[:, None] * stride_row + offs_n[None, :] * stride_col\n",
        "    w_q = tl.load(weight_tile_ptr, mask=mask, other=8)\n",
        "\n",
        "    # Convert [0,15] range to [-8,7] range\n",
        "    w_centered = tl.cast(w_q, tl.int32) - 8\n",
        "\n",
        "    # Load per-row absmax scaling factors\n",
        "    absmax = tl.load(absmax_ptr + offs_m, mask=(offs_m < M), other=0.0)\n",
        "    absmax_fp16 = tl.cast(absmax, tl.float16)\n",
        "\n",
        "    # Convert weights to fp16 and apply scaling\n",
        "    w_centered_fp16 = tl.cast(w_centered, tl.float16)\n",
        "    deq = (w_centered_fp16 * absmax_fp16[:, None]) / 7.0\n",
        "\n",
        "    # Store dequantized tile\n",
        "    out_ptr_tile = out_ptr + offs_m[:, None] * stride_row + offs_n[None, :] * stride_col\n",
        "    tl.store(out_ptr_tile, deq, mask=mask)\n",
        "\n",
        "\n",
        "\n",
        "# Wrapper function\n",
        "def nf4_to_fp16(nf4, absmax):\n",
        "    M, N = nf4.shape\n",
        "    out = torch.empty((M, N), device=nf4.device, dtype=torch.float16)\n",
        "    grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n",
        "    nf4_dequant_kernel[grid](\n",
        "        nf4, absmax, out,\n",
        "        M, N,\n",
        "        nf4.stride(0), nf4.stride(1),\n",
        "        BLOCK_M=64, BLOCK_N=64\n",
        "    )\n",
        "    return out\n",
        "\n",
        "\n",
        "# Testing the fix\n",
        "def test_dequantize_function():\n",
        "    M, N = 1024, 1024\n",
        "    nf4 = torch.randint(0, 16, (M, N), device='cuda', dtype=torch.int8)\n",
        "    absmax = (torch.rand(M, device='cuda', dtype=torch.float16) * 0.9 + 0.1)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    out_triton = nf4_to_fp16(nf4, absmax)\n",
        "    torch.cuda.synchronize()\n",
        "    out_ref = ((nf4.to(torch.int32) - 8).to(torch.float16) * absmax.unsqueeze(1)) / 7.0\n",
        "\n",
        "    print(\"Results match:\", torch.allclose(out_triton, out_ref, atol=1e-2))\n",
        "\n",
        "    # Benchmark\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    for _ in range(10):\n",
        "        nf4_to_fp16(nf4, absmax)\n",
        "    torch.cuda.synchronize()\n",
        "    triton_time = (time.time() - t0) / 10.0\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    for _ in range(10):\n",
        "        out_ref = ((nf4.to(torch.int32) - 8).to(torch.float16) * absmax.unsqueeze(1)) / 7.0\n",
        "    torch.cuda.synchronize()\n",
        "    ref_time = (time.time() - t0) / 10.0\n",
        "\n",
        "    print(f\"Triton Speedup: {ref_time / triton_time:.2f}x\")\n",
        "\n",
        "# Run test\n",
        "test_dequantize_function()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade triton"
      ],
      "metadata": {
        "id": "OuvTbuP1fFos",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "4677f23f-2ffc-4842-bcaf-c4ddff8a97da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Collecting triton\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.0.0\n",
            "    Uninstalling triton-2.0.0:\n",
            "      Successfully uninstalled triton-2.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.0.1 requires triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.2.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed triton-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "triton"
                ]
              },
              "id": "afeacaefda3243658b2a5eeeff12754b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPROVED SPEED TO 1.51X"
      ],
      "metadata": {
        "id": "m9SStabt6euh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import time\n",
        "\n",
        "@triton.jit\n",
        "def nf4_dequant_kernel(weight_ptr, absmax_ptr, out_ptr,\n",
        "                       M: int, N: int, stride_row: int, stride_col: int,\n",
        "                       BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "\n",
        "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n",
        "\n",
        "    weight_tile_ptr = weight_ptr + offs_m[:, None] * stride_row + offs_n[None, :] * stride_col\n",
        "    w_q = tl.load(weight_tile_ptr, mask=mask, other=8)\n",
        "\n",
        "    w_centered = tl.cast(w_q, tl.int32) - 8\n",
        "\n",
        "    absmax = tl.load(absmax_ptr + offs_m, mask=(offs_m < M), other=0.0)\n",
        "    absmax_fp16 = tl.cast(absmax, tl.float16)\n",
        "\n",
        "    w_centered_fp16 = tl.cast(w_centered, tl.float16)\n",
        "    deq = (w_centered_fp16 * absmax_fp16[:, None]) / 7.0\n",
        "\n",
        "    out_ptr_tile = out_ptr + offs_m[:, None] * stride_row + offs_n[None, :] * stride_col\n",
        "    tl.store(out_ptr_tile, deq, mask=mask)\n",
        "\n",
        "def nf4_to_fp16(nf4, absmax):\n",
        "    M, N = nf4.shape\n",
        "    out = torch.empty((M, N), device=nf4.device, dtype=torch.float16)\n",
        "    grid = (triton.cdiv(M, 128), triton.cdiv(N, 128))  # Adjusted block sizes\n",
        "    nf4_dequant_kernel[grid](\n",
        "        nf4, absmax, out,\n",
        "        M, N,\n",
        "        nf4.stride(0), nf4.stride(1),\n",
        "        BLOCK_M=128, BLOCK_N=128  # Adjusted block sizes\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def test_dequantize_function():\n",
        "    M, N = 1024, 1024\n",
        "    nf4 = torch.randint(0, 16, (M, N), device='cuda', dtype=torch.int8)\n",
        "    absmax = (torch.rand(M, device='cuda', dtype=torch.float16) * 0.9 + 0.1)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    out_triton = nf4_to_fp16(nf4, absmax)\n",
        "    torch.cuda.synchronize()\n",
        "    out_ref = ((nf4.to(torch.int32) - 8).to(torch.float16) * absmax.unsqueeze(1)) / 7.0\n",
        "\n",
        "    print(\"Results match:\", torch.allclose(out_triton, out_ref, atol=1e-2))\n",
        "\n",
        "    # Benchmark\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    for _ in range(10):\n",
        "        nf4_to_fp16(nf4, absmax)\n",
        "    torch.cuda.synchronize()\n",
        "    triton_time = (time.time() - t0) / 10.0\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    for _ in range(10):\n",
        "        out_ref = ((nf4.to(torch.int32) - 8).to(torch.float16) * absmax.unsqueeze(1)) / 7.0\n",
        "    torch.cuda.synchronize()\n",
        "    ref_time = (time.time() - t0) / 10.0\n",
        "\n",
        "    print(f\"Triton Speedup: {ref_time / triton_time:.2f}x\")\n",
        "\n",
        "# Run test\n",
        "test_dequantize_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTWBQIn1yZMC",
        "outputId": "f44b8a27-15a8-4b77-a7ee-c219999e9e47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match: True\n",
            "Triton Speedup: 1.51x\n"
          ]
        }
      ]
    }
  ]
}